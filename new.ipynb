{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea45b6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 14:41:05.625153: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-05 14:41:05.656197: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-05 14:41:06.283585: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from equation.option_pricing import BlackScholes\n",
    "from optimize.option_princing import BlackScholeOptimizer\n",
    "from method.nn import MLP, ResNet\n",
    "from method.hnn import HybridCQN\n",
    "from method.qnn import QuantumNeuralNetwork, CorrelatorQuantumNeuralNetwork\n",
    "from utils.adapters import ResNetFeatures, MLPFeatures\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch as tc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "770ba1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.8.0+cu128\n",
      "CUDA runtime: 12.8\n",
      "cuda available? True\n",
      "device name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"CUDA runtime:\", torch.version.cuda)\n",
    "print(\"cuda available?\", torch.cuda.is_available())\n",
    "print(\"device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9a5728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) escolha do device (uma string é suficiente para nossas classes)\n",
    "device = \"cpu\"   # \"cuda\" para forçar GPU, \"cpu\" para CPU, \"auto\" escolhe GPU se houver\n",
    "dtype  = tc.float32\n",
    "\n",
    "# 2) dados (sem mudanças)\n",
    "bse = BlackScholes(eps=1e-10)\n",
    "data = bse.generate_data()\n",
    "data_teste = bse.generate_data(seed=42)\n",
    "\n",
    "epochs = 10\n",
    "lr = 2e-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3217f2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainning: 100%|██████████| 10/10 [00:00<00:00, 56.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# 3) MLP clássico (ajuste os nomes dos args: era hidden/blocks)\n",
    "model = MLP(hidden=4, blocks=5, activation=nn.Tanh(),\n",
    "            device=device)\n",
    "\n",
    "opt = BlackScholeOptimizer(data, model,\n",
    "                           epochs=epochs, lr=lr,\n",
    "                           device=device)\n",
    "\n",
    "loss = opt.train(return_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "201c111f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainning: 100%|██████████| 10/10 [00:02<00:00,  4.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# 4) QNN + híbrido\n",
    "qnn = QuantumNeuralNetwork(n_qubits=5, n_layers=4,\n",
    "                           device=device, dtype=dtype)\n",
    "\n",
    "hybrid = HybridCQN(classical_pre=model, qnn_block=qnn, classical_post=None,\n",
    "                   input_dim=2, output_dim=1,\n",
    "                   device=device, dtype=dtype)\n",
    "\n",
    "qpinn_opt = BlackScholeOptimizer(data, hybrid,\n",
    "                                 epochs=epochs, lr=lr,\n",
    "                                 device=device, dtype=dtype)\n",
    "\n",
    "loss_qpinn = qpinn_opt.train(return_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b972761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in hybrid.named_parameters():\n",
    "    print(name, param.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51e899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bse = BlackScholes(eps=1e-10)\n",
    "data = bse.generate_data()\n",
    "data_teste = bse.generate_data(seed=42)\n",
    "epochs = 1000\n",
    "lr = 2e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f6d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(neurons=4, M=5, activation=nn.Tanh())\n",
    "opt = BlackScholeOptimizer(data, model, epochs=epochs, lr=lr)\n",
    "loss = opt.train(return_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c640d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model = ResNet(hidden=4, blocks=5, activation=nn.Tanh())\n",
    "res_opt = BlackScholeOptimizer(data, res_model, epochs=epochs, lr=lr)\n",
    "loss_ress = res_opt.train(return_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd14ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "qnn = QuantumNeuralNetwork(n_qubits=5, n_layers=4)\n",
    "qpinn_opt = BlackScholeOptimizer(data, HybridCQN(classical_pre=None, qnn_block=qnn, classical_post=None), epochs=epochs, lr=lr)\n",
    "loss_qpinn= qpinn_opt.train(return_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b84b2486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainning:   0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got mat1 is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m hybrid = HybridCQN(classical_pre=model, qnn_block=cqnn, classical_post=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m      3\u001b[39m cqpinn_opt = BlackScholeOptimizer(data, hybrid, epochs=epochs, lr=lr)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m loss_cqpinn= \u001b[43mcqpinn_opt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/pinn/optimize/option_princing.py:260\u001b[39m, in \u001b[36mBlackScholeOptimizer.train\u001b[39m\u001b[34m(self, S_max, T, V_max, normalize, return_loss, return_all)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _t.requires_grad: _t.requires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.epochs), desc=\u001b[33m\"\u001b[39m\u001b[33mTrainning\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     Vpred    = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_S\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[43m_t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m     VTpred   = \u001b[38;5;28mself\u001b[39m.model(tc.cat([_ST,   _tT],   dim=\u001b[32m1\u001b[39m))\n\u001b[32m    262\u001b[39m     V0pred   = \u001b[38;5;28mself\u001b[39m.model(tc.cat([_S0,   _t0],   dim=\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/pinn/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/pinn/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/pinn/method/hnn.py:62\u001b[39m, in \u001b[36mHybridCQN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# ===== NOVO: garantir device/dtype da entrada =====\u001b[39;00m\n\u001b[32m     60\u001b[39m     x = x.to(\u001b[38;5;28mself\u001b[39m._torch_device, dtype=\u001b[38;5;28mself\u001b[39m._dtype)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     h = x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pre \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpre\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# (B, F)\u001b[39;00m\n\u001b[32m     63\u001b[39m     q_in = \u001b[38;5;28mself\u001b[39m.to_qubits(h)                     \u001b[38;5;66;03m# (B, n_qubits)  -- sem ativação\u001b[39;00m\n\u001b[32m     64\u001b[39m     q_out = \u001b[38;5;28mself\u001b[39m.qnn(q_in)                       \u001b[38;5;66;03m# (B, n_qubits)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/pinn/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/pinn/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/pinn/method/nn.py:25\u001b[39m, in \u001b[36mMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     23\u001b[39m x = x.to(\u001b[38;5;28mself\u001b[39m._torch_device, dtype=\u001b[38;5;28mself\u001b[39m._dtype)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hidden_layers:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.activation(\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output_layer(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/pinn/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/pinn/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PhD/pinn/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but got mat1 is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "cqnn = CorrelatorQuantumNeuralNetwork(n_qubits=3, n_layers=4, k=2, n_vertex=5)\n",
    "hybrid = HybridCQN(classical_pre=model, qnn_block=cqnn, classical_post=None)\n",
    "cqpinn_opt = BlackScholeOptimizer(data, hybrid, epochs=epochs, lr=lr)\n",
    "loss_cqpinn= cqpinn_opt.train(return_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76c29623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre.hidden_layers.0.weight cuda:0\n",
      "pre.hidden_layers.0.bias cuda:0\n",
      "pre.hidden_layers.1.weight cuda:0\n",
      "pre.hidden_layers.1.bias cuda:0\n",
      "pre.hidden_layers.2.weight cuda:0\n",
      "pre.hidden_layers.2.bias cuda:0\n",
      "pre.hidden_layers.3.weight cuda:0\n",
      "pre.hidden_layers.3.bias cuda:0\n",
      "pre.hidden_layers.4.weight cuda:0\n",
      "pre.hidden_layers.4.bias cuda:0\n",
      "pre.output_layer.weight cuda:0\n",
      "pre.output_layer.bias cuda:0\n",
      "qnn.q_layer.weights cuda:0\n",
      "to_qubits.weight cuda:0\n",
      "to_qubits.bias cuda:0\n",
      "q_out.weight cuda:0\n",
      "q_out.bias cuda:0\n"
     ]
    }
   ],
   "source": [
    "for name, param in hybrid.named_parameters():\n",
    "    print(name, param.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfcaa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cqnn_sem = CorrelatorQuantumNeuralNetwork(n_qubits=3, n_layers=4, k=2, n_vertex=5, nonlinear=None)\n",
    "cqpinn_opt_sem = BlackScholeOptimizer(data, HybridCQN(classical_pre=None, qnn_block=cqnn_sem, classical_post=None), epochs=epochs, lr=lr)\n",
    "loss_cqpinn_sem= cqpinn_opt_sem.train(return_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1f827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss['Total'], lw=2.5,label='PINN - MLP')\n",
    "#plt.plot(loss_ress['Total'], lw=2.5,label='PINN - ResNet')\n",
    "plt.plot(loss_qpinn['Total'], lw=2.5,label='QPINN')\n",
    "plt.plot(loss_cqpinn['Total'], lw=2.5,label='CQPINN')\n",
    "plt.plot(loss_cqpinn_sem['Total'], lw=2.5,label='CQPINN - Without') \n",
    "#plt.plot(loss_hpinn['Total'], lw=2.5,label='HPINN - MLP')\n",
    "#plt.plot(loss_hpinn_res['Total'], lw=2.5,label='HPINN - ResNet')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpinn_res_opt = BlackScholeOptimizer(data, HybridCQN(classical_pre=ResNetFeatures(ResNet(hidden=4, blocks=1, activation=nn.Tanh())), qnn_block=qnn, classical_post=None), epochs=epochs, lr=lr)\n",
    "_, loss_hpinn_res = hpinn_res_opt.train(return_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b49c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hpinn_opt = BlackScholeOptimizer(data, HybridCQN(classical_pre=MLPFeatures(MLP(neurons=4, M=1, activation=nn.Tanh())), qnn_block=qnn, classical_post=None), epochs=epochs, lr=lr)\n",
    "_, loss_hpinn = hpinn_opt.train(return_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.num_params, res_opt.num_params, qpinn_opt.num_params, hpinn_res_opt.num_params, hpinn_opt.num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e446d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results for each method\n",
    "results = {\n",
    "    'PINN-MLP': {'losses': [], 'best_loss': float('inf'), 'best_history': None},\n",
    "    'PINN-ResNet': {'losses': [], 'best_loss': float('inf'), 'best_history': None},\n",
    "    'QPINN': {'losses': [], 'best_loss': float('inf'), 'best_history': None},\n",
    "    'HPINN-MLP': {'losses': [], 'best_loss': float('inf'), 'best_history': None},\n",
    "    'HPINN-ResNet': {'losses': [], 'best_loss': float('inf'), 'best_history': None}\n",
    "}\n",
    "\n",
    "n_iterations = 10\n",
    "print(f\"Training each model {n_iterations} times...\")\n",
    "\n",
    "for i in tqdm(range(n_iterations)):\n",
    "    n_iterations = 10\n",
    "    print(f\"Training each model {i}...\")\n",
    "    # PINN-MLP\n",
    "    model = MLP(neurons=4, M=5, activation=nn.Tanh())\n",
    "    opt = BlackScholeOptimizer(data, model, epochs=epochs, lr=lr)\n",
    "    _, current_loss = opt.train(return_loss=True)\n",
    "    avg_loss = np.mean(current_loss['Total'][-100:])\n",
    "    results['PINN-MLP']['losses'].append(current_loss['Total'])\n",
    "    if avg_loss < results['PINN-MLP']['best_loss']:\n",
    "        results['PINN-MLP']['best_loss'] = avg_loss\n",
    "        results['PINN-MLP']['best_history'] = current_loss['Total']\n",
    "    \n",
    "    # PINN-ResNet\n",
    "    res_model = ResNet(hidden=4, blocks=5, activation=nn.Tanh())\n",
    "    res_opt = BlackScholeOptimizer(data, res_model, epochs=epochs, lr=lr)\n",
    "    _, current_loss = res_opt.train(return_loss=True)\n",
    "    avg_loss = np.mean(current_loss['Total'][-100:])\n",
    "    results['PINN-ResNet']['losses'].append(current_loss['Total'])\n",
    "    if avg_loss < results['PINN-ResNet']['best_loss']:\n",
    "        results['PINN-ResNet']['best_loss'] = avg_loss\n",
    "        results['PINN-ResNet']['best_history'] = current_loss['Total']\n",
    "    \n",
    "    # QPINN\n",
    "    qnn = QuantumNeuralNetwork(n_qubits=4, n_layers=4)\n",
    "    qpinn_opt = BlackScholeOptimizer(data, HybridCQN(classical_pre=None, qnn_block=qnn, classical_post=None), epochs=epochs, lr=lr)\n",
    "    _, current_loss = qpinn_opt.train(return_loss=True)\n",
    "    avg_loss = np.mean(current_loss['Total'][-100:])\n",
    "    results['QPINN']['losses'].append(current_loss['Total'])\n",
    "    if avg_loss < results['QPINN']['best_loss']:\n",
    "        results['QPINN']['best_loss'] = avg_loss\n",
    "        results['QPINN']['best_history'] = current_loss['Total']\n",
    "    \n",
    "    # HPINN-ResNet\n",
    "    qnn = QuantumNeuralNetwork(n_qubits=4, n_layers=3)\n",
    "    hpinn_res_opt = BlackScholeOptimizer(data, HybridCQN(classical_pre=ResNetFeatures(ResNet(hidden=4, blocks=2, activation=nn.Tanh())), \n",
    "                                                         qnn_block=qnn, classical_post=None), epochs=epochs, lr=lr)\n",
    "    _, current_loss = hpinn_res_opt.train(return_loss=True)\n",
    "    avg_loss = np.mean(current_loss['Total'][-100:])\n",
    "    results['HPINN-ResNet']['losses'].append(current_loss['Total'])\n",
    "    if avg_loss < results['HPINN-ResNet']['best_loss']:\n",
    "        results['HPINN-ResNet']['best_loss'] = avg_loss\n",
    "        results['HPINN-ResNet']['best_history'] = current_loss['Total']\n",
    "    \n",
    "    # HPINN-MLP\n",
    "    hpinn_opt = BlackScholeOptimizer(data, HybridCQN(classical_pre=MLPFeatures(MLP(neurons=4, M=1, activation=nn.Tanh())), \n",
    "                                                     qnn_block=qnn, classical_post=None), epochs=epochs, lr=lr)\n",
    "    _, current_loss = hpinn_opt.train(return_loss=True)\n",
    "    avg_loss = np.mean(current_loss['Total'][-100:])\n",
    "    results['HPINN-MLP']['losses'].append(current_loss['Total'])\n",
    "    if avg_loss < results['HPINN-MLP']['best_loss']:\n",
    "        results['HPINN-MLP']['best_loss'] = avg_loss\n",
    "        results['HPINN-MLP']['best_history'] = current_loss['Total']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc81c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot best losses\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "for method in results:\n",
    "    plt.plot(results[method]['best_history'], lw=2.5, label=f'{method}\\n(Best={results[method][\"best_history\"][-1]:.2e})')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Best Loss History', fontsize=14)\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot average losses with percentile bands\n",
    "plt.subplot(1, 2, 2)\n",
    "for method in results:\n",
    "    losses_array = np.array(results[method]['losses'])\n",
    "    median = np.median(losses_array, axis=0)\n",
    "    q25 = np.percentile(losses_array, 25, axis=0)\n",
    "    q75 = np.percentile(losses_array, 75, axis=0)\n",
    "    \n",
    "    plt.plot(median, lw=2.5, label=method)\n",
    "    plt.fill_between(range(len(median)), q25, q75, alpha=0.2)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Median Loss with 25-75 Percentile Band', fontsize=14)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ecfda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a62d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_num =opt.num_params, res_opt.num_params, qpinn_opt.num_params, hpinn_res_opt.num_params, hpinn_opt.num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd223ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_num[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ce09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = []\n",
    "for i, method in enumerate(results):\n",
    "    teste.append(results[method][\"best_loss\"]/param_num[i])\n",
    "    print(f\"{method}:\", results[method][\"best_loss\"]/param_num[i], results[method][\"best_history\"][-1]/param_num[i])\n",
    "\n",
    "print(teste/min(teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88699aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistical summary\n",
    "print(\"\\nStatistical Summary (based on final 100 epochs average):\")\n",
    "for method in results:\n",
    "    final_losses = np.array([np.mean(loss[-100:]) for loss in results[method]['losses']])\n",
    "    q25, median, q75 = np.percentile(final_losses, [25, 50, 75])\n",
    "    iqr = q75 - q25\n",
    "    print(f\"\\n{method}:\")\n",
    "    print(f\"Median: {median:.2e}\")\n",
    "    print(f\"Q25-Q75: [{q25:.2e} - {q75:.2e}]\")\n",
    "    print(f\"IQR: {iqr:.2e}\")\n",
    "    print(f\"Stability (IQR/Median): {(iqr/median):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
